{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMzDgKAMWZJzp1QzAph2XkY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"3e9bbf9d387045edb5d01e973da08fad":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d8a6cb5333e34d788f1f93642f1b04e4","IPY_MODEL_6890bdcf56bd4cebba48ec803042af10","IPY_MODEL_7f4860a57674425a883eb1bdbd25ec1c"],"layout":"IPY_MODEL_9522e7cd7d1c4b2c8d84789f9a9ab80c"}},"d8a6cb5333e34d788f1f93642f1b04e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1deb8e088dac447293c3e62c70f9ab59","placeholder":"​","style":"IPY_MODEL_7995c76dbab5405d9155c11813a33cc1","value":"100%"}},"6890bdcf56bd4cebba48ec803042af10":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ba10d77f1ba49cf8d3003ff27371611","max":102530333,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fe575afb409f4e52a8dd6b3e62d0748a","value":102530333}},"7f4860a57674425a883eb1bdbd25ec1c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_497aa3fa5ff14885ab83496204784339","placeholder":"​","style":"IPY_MODEL_e27f017f3e9540908ee37b8148440e64","value":" 97.8M/97.8M [00:00&lt;00:00, 193MB/s]"}},"9522e7cd7d1c4b2c8d84789f9a9ab80c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1deb8e088dac447293c3e62c70f9ab59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7995c76dbab5405d9155c11813a33cc1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ba10d77f1ba49cf8d3003ff27371611":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe575afb409f4e52a8dd6b3e62d0748a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"497aa3fa5ff14885ab83496204784339":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e27f017f3e9540908ee37b8148440e64":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ytOkLHk2ill","executionInfo":{"status":"ok","timestamp":1678725867099,"user_tz":-540,"elapsed":32064,"user":{"displayName":"김도훈","userId":"04279245867298114899"}},"outputId":"e83917b7-ef31-490a-dcaf-f33d3d270e9f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Colab Notebooks/dna/week6"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mcpy6EFE3tN8","executionInfo":{"status":"ok","timestamp":1678726064333,"user_tz":-540,"elapsed":21,"user":{"displayName":"김도훈","userId":"04279245867298114899"}},"outputId":"1febd03d-077e-41f7-aa97-07894536c433"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/dna/week6\n"]}]},{"cell_type":"code","source":["import torch\n","device = 'cuda' if torch.cuda.is_available() else 'cpu' # device 배정\n","torch.manual_seed(42)\n","if device == 'cuda':\n","  torch.cuda.manual_seed_all(42)\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"RuxY5Wra2l4a","executionInfo":{"status":"ok","timestamp":1678725870585,"user_tz":-540,"elapsed":3497,"user":{"displayName":"김도훈","userId":"04279245867298114899"}},"outputId":"d874deb3-e841-4814-f151-17139d397dba"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["# 1. 데이터 셋 준비하기"],"metadata":{"id":"b17Spefa36Ut"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","# 하이퍼 파라미터\n","batch_size = 8\n","lr = 0.0001\n","epochs = 100\n","optimizer_name = 'adam'\n","model_name = 'resnet50'\n","criterion = nn.CrossEntropyLoss().to(device) # cost function"],"metadata":{"id":"ehncdWFK2ryP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from dataset import Custom_dataset as C\n","from torch.utils.data import Dataset, DataLoader\n","import cv2\n","import os \n","import torch\n","import torchvision\n","from torchvision import transforms # 이미지 데이터 augmentation\n","import glob\n","import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2 # albumentations 텐서화 함수\n","\n","root_path = '/content/drive/MyDrive/Colab Notebooks/dna/week6/original-1'\n","\n","train_transforms = A.Compose([\n","    A.Resize(224,224),\n","    A.Transpose(p=0.5),\n","    A.HorizontalFlip(p=0.5),\n","    A.VerticalFlip(p=0.5),\n","    A.ShiftScaleRotate(p=0.5),\n","    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=20, val_shift_limit=20, p=0.5),\n","    A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n","    A.ChannelShuffle(),\n","    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0), # 이미지넷 데이터셋 통계값으로 Normalize\n","    A.CoarseDropout(p=0.5),\n","    ToTensorV2()\n","])\n","\n","test_transforms = A.Compose([\n","    A.Resize(224,224),\n","    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0), # 텐서타입은 안해줌\n","    ToTensorV2() # Normalize를 먼저하고 tensor화를 진행해야한다.\n","])\n","\n","### Pytorch 데이터 클래스 생성\n","train_class = C(root_path=root_path, mode='train', transforms=train_transforms)\n","valid_class = C(root_path=root_path, mode='valid', transforms=test_transforms)\n","test_class = C(root_path=root_path, mode='test', transforms=test_transforms)\n","\n","### Pytorch BatchLoader 생성 (학습에 이용할 최종 dataloader)\n","# from torch.utils.data import DataLoader as DataLoader\n","\n","train_loader = DataLoader(train_class, batch_size=batch_size, shuffle = True, num_workers=0)\n","valid_loader = DataLoader(valid_class, batch_size=batch_size, shuffle = False, num_workers=0)\n","test_loader = DataLoader(test_class, batch_size=batch_size, shuffle = False, num_workers=0)"],"metadata":{"id":"s0RekuIH21Te"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.모델 불러오기"],"metadata":{"id":"620jLNJ739rm"}},{"cell_type":"code","source":["from torchvision import models # 모델 라이브러리 함수\n","\n","resnet_50 = models.resnet50(pretrained=True).to(device) # 선행학습 여부\n","\n","# finetuning\n","import torch.nn as nn # 파이토치 뉴럴네트워크 layer 라이브러리\n","resnet_50.fc = nn.Linear(resnet_50.fc.in_features, 3).to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":162,"referenced_widgets":["3e9bbf9d387045edb5d01e973da08fad","d8a6cb5333e34d788f1f93642f1b04e4","6890bdcf56bd4cebba48ec803042af10","7f4860a57674425a883eb1bdbd25ec1c","9522e7cd7d1c4b2c8d84789f9a9ab80c","1deb8e088dac447293c3e62c70f9ab59","7995c76dbab5405d9155c11813a33cc1","2ba10d77f1ba49cf8d3003ff27371611","fe575afb409f4e52a8dd6b3e62d0748a","497aa3fa5ff14885ab83496204784339","e27f017f3e9540908ee37b8148440e64"]},"id":"gxx6ECYy3yko","executionInfo":{"status":"ok","timestamp":1678726144816,"user_tz":-540,"elapsed":6133,"user":{"displayName":"김도훈","userId":"04279245867298114899"}},"outputId":"1a436acd-2575-4f63-bdbd-3aff1bca3442"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/97.8M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e9bbf9d387045edb5d01e973da08fad"}},"metadata":{}}]},{"cell_type":"code","source":["from torchsummary import summary # 모델 아키텍쳐 확인하는 함수\n","\n","summary(resnet_50, input_size = (3, 224, 224))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WCKloF_x4Dki","executionInfo":{"status":"ok","timestamp":1678726159434,"user_tz":-540,"elapsed":5098,"user":{"displayName":"김도훈","userId":"04279245867298114899"}},"outputId":"114218d3-7b9a-413c-9b32-479751605419"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 112, 112]           9,408\n","       BatchNorm2d-2         [-1, 64, 112, 112]             128\n","              ReLU-3         [-1, 64, 112, 112]               0\n","         MaxPool2d-4           [-1, 64, 56, 56]               0\n","            Conv2d-5           [-1, 64, 56, 56]           4,096\n","       BatchNorm2d-6           [-1, 64, 56, 56]             128\n","              ReLU-7           [-1, 64, 56, 56]               0\n","            Conv2d-8           [-1, 64, 56, 56]          36,864\n","       BatchNorm2d-9           [-1, 64, 56, 56]             128\n","             ReLU-10           [-1, 64, 56, 56]               0\n","           Conv2d-11          [-1, 256, 56, 56]          16,384\n","      BatchNorm2d-12          [-1, 256, 56, 56]             512\n","           Conv2d-13          [-1, 256, 56, 56]          16,384\n","      BatchNorm2d-14          [-1, 256, 56, 56]             512\n","             ReLU-15          [-1, 256, 56, 56]               0\n","       Bottleneck-16          [-1, 256, 56, 56]               0\n","           Conv2d-17           [-1, 64, 56, 56]          16,384\n","      BatchNorm2d-18           [-1, 64, 56, 56]             128\n","             ReLU-19           [-1, 64, 56, 56]               0\n","           Conv2d-20           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-21           [-1, 64, 56, 56]             128\n","             ReLU-22           [-1, 64, 56, 56]               0\n","           Conv2d-23          [-1, 256, 56, 56]          16,384\n","      BatchNorm2d-24          [-1, 256, 56, 56]             512\n","             ReLU-25          [-1, 256, 56, 56]               0\n","       Bottleneck-26          [-1, 256, 56, 56]               0\n","           Conv2d-27           [-1, 64, 56, 56]          16,384\n","      BatchNorm2d-28           [-1, 64, 56, 56]             128\n","             ReLU-29           [-1, 64, 56, 56]               0\n","           Conv2d-30           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-31           [-1, 64, 56, 56]             128\n","             ReLU-32           [-1, 64, 56, 56]               0\n","           Conv2d-33          [-1, 256, 56, 56]          16,384\n","      BatchNorm2d-34          [-1, 256, 56, 56]             512\n","             ReLU-35          [-1, 256, 56, 56]               0\n","       Bottleneck-36          [-1, 256, 56, 56]               0\n","           Conv2d-37          [-1, 128, 56, 56]          32,768\n","      BatchNorm2d-38          [-1, 128, 56, 56]             256\n","             ReLU-39          [-1, 128, 56, 56]               0\n","           Conv2d-40          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-41          [-1, 128, 28, 28]             256\n","             ReLU-42          [-1, 128, 28, 28]               0\n","           Conv2d-43          [-1, 512, 28, 28]          65,536\n","      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n","           Conv2d-45          [-1, 512, 28, 28]         131,072\n","      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n","             ReLU-47          [-1, 512, 28, 28]               0\n","       Bottleneck-48          [-1, 512, 28, 28]               0\n","           Conv2d-49          [-1, 128, 28, 28]          65,536\n","      BatchNorm2d-50          [-1, 128, 28, 28]             256\n","             ReLU-51          [-1, 128, 28, 28]               0\n","           Conv2d-52          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-53          [-1, 128, 28, 28]             256\n","             ReLU-54          [-1, 128, 28, 28]               0\n","           Conv2d-55          [-1, 512, 28, 28]          65,536\n","      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n","             ReLU-57          [-1, 512, 28, 28]               0\n","       Bottleneck-58          [-1, 512, 28, 28]               0\n","           Conv2d-59          [-1, 128, 28, 28]          65,536\n","      BatchNorm2d-60          [-1, 128, 28, 28]             256\n","             ReLU-61          [-1, 128, 28, 28]               0\n","           Conv2d-62          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-63          [-1, 128, 28, 28]             256\n","             ReLU-64          [-1, 128, 28, 28]               0\n","           Conv2d-65          [-1, 512, 28, 28]          65,536\n","      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n","             ReLU-67          [-1, 512, 28, 28]               0\n","       Bottleneck-68          [-1, 512, 28, 28]               0\n","           Conv2d-69          [-1, 128, 28, 28]          65,536\n","      BatchNorm2d-70          [-1, 128, 28, 28]             256\n","             ReLU-71          [-1, 128, 28, 28]               0\n","           Conv2d-72          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-73          [-1, 128, 28, 28]             256\n","             ReLU-74          [-1, 128, 28, 28]               0\n","           Conv2d-75          [-1, 512, 28, 28]          65,536\n","      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n","             ReLU-77          [-1, 512, 28, 28]               0\n","       Bottleneck-78          [-1, 512, 28, 28]               0\n","           Conv2d-79          [-1, 256, 28, 28]         131,072\n","      BatchNorm2d-80          [-1, 256, 28, 28]             512\n","             ReLU-81          [-1, 256, 28, 28]               0\n","           Conv2d-82          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-83          [-1, 256, 14, 14]             512\n","             ReLU-84          [-1, 256, 14, 14]               0\n","           Conv2d-85         [-1, 1024, 14, 14]         262,144\n","      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n","           Conv2d-87         [-1, 1024, 14, 14]         524,288\n","      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n","             ReLU-89         [-1, 1024, 14, 14]               0\n","       Bottleneck-90         [-1, 1024, 14, 14]               0\n","           Conv2d-91          [-1, 256, 14, 14]         262,144\n","      BatchNorm2d-92          [-1, 256, 14, 14]             512\n","             ReLU-93          [-1, 256, 14, 14]               0\n","           Conv2d-94          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-95          [-1, 256, 14, 14]             512\n","             ReLU-96          [-1, 256, 14, 14]               0\n","           Conv2d-97         [-1, 1024, 14, 14]         262,144\n","      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n","             ReLU-99         [-1, 1024, 14, 14]               0\n","      Bottleneck-100         [-1, 1024, 14, 14]               0\n","          Conv2d-101          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-102          [-1, 256, 14, 14]             512\n","            ReLU-103          [-1, 256, 14, 14]               0\n","          Conv2d-104          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-105          [-1, 256, 14, 14]             512\n","            ReLU-106          [-1, 256, 14, 14]               0\n","          Conv2d-107         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n","            ReLU-109         [-1, 1024, 14, 14]               0\n","      Bottleneck-110         [-1, 1024, 14, 14]               0\n","          Conv2d-111          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-112          [-1, 256, 14, 14]             512\n","            ReLU-113          [-1, 256, 14, 14]               0\n","          Conv2d-114          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-115          [-1, 256, 14, 14]             512\n","            ReLU-116          [-1, 256, 14, 14]               0\n","          Conv2d-117         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n","            ReLU-119         [-1, 1024, 14, 14]               0\n","      Bottleneck-120         [-1, 1024, 14, 14]               0\n","          Conv2d-121          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-122          [-1, 256, 14, 14]             512\n","            ReLU-123          [-1, 256, 14, 14]               0\n","          Conv2d-124          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-125          [-1, 256, 14, 14]             512\n","            ReLU-126          [-1, 256, 14, 14]               0\n","          Conv2d-127         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n","            ReLU-129         [-1, 1024, 14, 14]               0\n","      Bottleneck-130         [-1, 1024, 14, 14]               0\n","          Conv2d-131          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-132          [-1, 256, 14, 14]             512\n","            ReLU-133          [-1, 256, 14, 14]               0\n","          Conv2d-134          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-135          [-1, 256, 14, 14]             512\n","            ReLU-136          [-1, 256, 14, 14]               0\n","          Conv2d-137         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n","            ReLU-139         [-1, 1024, 14, 14]               0\n","      Bottleneck-140         [-1, 1024, 14, 14]               0\n","          Conv2d-141          [-1, 512, 14, 14]         524,288\n","     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n","            ReLU-143          [-1, 512, 14, 14]               0\n","          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n","            ReLU-146            [-1, 512, 7, 7]               0\n","          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n","     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n","          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n","     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n","            ReLU-151           [-1, 2048, 7, 7]               0\n","      Bottleneck-152           [-1, 2048, 7, 7]               0\n","          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n","     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n","            ReLU-155            [-1, 512, 7, 7]               0\n","          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n","            ReLU-158            [-1, 512, 7, 7]               0\n","          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n","     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n","            ReLU-161           [-1, 2048, 7, 7]               0\n","      Bottleneck-162           [-1, 2048, 7, 7]               0\n","          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n","     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n","            ReLU-165            [-1, 512, 7, 7]               0\n","          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n","            ReLU-168            [-1, 512, 7, 7]               0\n","          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n","     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n","            ReLU-171           [-1, 2048, 7, 7]               0\n","      Bottleneck-172           [-1, 2048, 7, 7]               0\n","AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n","          Linear-174                    [-1, 3]           6,147\n","================================================================\n","Total params: 23,514,179\n","Trainable params: 23,514,179\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 286.55\n","Params size (MB): 89.70\n","Estimated Total Size (MB): 376.82\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["optimizer = torch.optim.Adam(resnet_50.parameters(), lr = lr, weight_decay = 1e-8)"],"metadata":{"id":"T_Nr1c7m4XQw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3,모델 학습시키고 모델저장"],"metadata":{"id":"t8meVx2h4HJE"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"xw57QyRF75YF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(1, epochs):\n","  running_loss = 0.0\n","  test_running_loss = 0.0\n","  total = 0\n","  correct = 0\n","  \n","  train_acc=0\n","  test_acc=0\n","  \n","  train_acc_lst = []\n","  train_loss_lst=[]\n","  test_acc_lst = []\n","  test_loss_lst=[]\n","\n","  save_dir = '/content/drive/MyDrive/Colab Notebooks/dna/week6/'\n","  model_name = 'resnet50'\n","\n","  resnet_50.train()\n","  for i, (train_img, train_label) in enumerate(train_loader):\n","    #gpu애 할당\n","    train_img = train_img.to(device)\n","    train_label = train_label.to(device)\n","    optimizer.zero_grad(set_to_none=True) #계산했던 가중치 초기화 #set_to_none넣으면 속도가 빨라짐\n","    \n","    output = resnet_50(train_img) #모델에 입력\n","    loss = criterion(output, train_label)\n","    loss.backward() #미분\n","    optimizer.step() #학습\n","\n","    #loss & acc\n","    running_loss += loss.item()\n","    _, predictions = torch.max(output.data , dim=1)\n","    \n","    total += train_label.size(0)\n","    correct += (predictions == train_label).sum().item()\n","    train_acc += 100*(correct/total)\n","    \n","  running_loss = round(running_loss/(i+1), 3) #소수점 반올림\n","  print(f\"trainset{epoch}/{epochs} Loss: {running_loss}, Accuracy: {train_acc/(i+1)}\")\n","  train_acc_lst.append(train_acc/(i+1))\n","  train_loss_lst.append(train_acc/(i+1))\n","\n","  resnet_50.eval()\n","  for i, (valid_img, valid_label) in enumerate(valid_loader):\n","    #gpu애 할당\n","    valid_img = valid_img.to(device)\n","    valid_label = valid_label.to(device)\n","    \n","    output = resnet_50(valid_img) #모델에 입력\n","    loss = criterion(output, valid_label)\n","\n","    #loss & acc\n","    test_running_loss += loss.item()\n","    _, predictions = torch.max(output.data , dim=1)\n","    \n","    total += valid_label.size(0)\n","    correct += (predictions == valid_label).sum().item()\n","    test_acc += 100*(correct/total)\n","\n","  test_running_loss = round(test_running_loss/(i+1), 3) #소수점 반올림\n","  print(f\"Validset{epoch}/{epochs} Loss: {test_running_loss}, Accuracy: {test_acc/(i+1)}\")\n","  test_acc_lst.append(test_acc/(i+1))\n","  test_loss_lst.append(test_running_loss)\n","    \n","  if(np.max(test_acc_lst) <= test_acc): #현재 에포크의 testacc가 가장 좋은 성능이라면,\n","    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n","    weights = resnet_50.state_dict() #지금 모델의 가중치를 저장한다.\n","    check_point = {\n","        'net' : weights,\n","\n","        'train_loss' : running_loss,\n","        'test loss' : test_running_loss,\n","        'epoch' : epoch,\n","\n","        'train_acc' : train_acc,\n","        'test_acc' : test_acc   \n","    }\n","    \n","    torch.save(check_point, save_dir + f'{model_name}.pth')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P8JTO_jU4DrB","executionInfo":{"status":"ok","timestamp":1678731076785,"user_tz":-540,"elapsed":1394124,"user":{"displayName":"김도훈","userId":"04279245867298114899"}},"outputId":"dbc29170-3373-4ba9-94ce-a5be1b1187fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["trainset1/100 Loss: 0.116, Accuracy: 96.65137319781024\n","Validset1/100 Loss: 0.517, Accuracy: 93.8433220018673\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset2/100 Loss: 0.108, Accuracy: 96.2148605761309\n","Validset2/100 Loss: 0.702, Accuracy: 95.37057696797127\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset3/100 Loss: 0.129, Accuracy: 95.85995186744077\n","Validset3/100 Loss: 0.6, Accuracy: 94.03454438301308\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset4/100 Loss: 0.111, Accuracy: 97.14072471031415\n","Validset4/100 Loss: 0.371, Accuracy: 95.96817135095102\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset5/100 Loss: 0.075, Accuracy: 97.45255039278553\n","Validset5/100 Loss: 0.336, Accuracy: 96.60950100890909\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset6/100 Loss: 0.053, Accuracy: 98.57455078891587\n","Validset6/100 Loss: 0.538, Accuracy: 96.4382802668851\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset7/100 Loss: 0.058, Accuracy: 97.9524602052669\n","Validset7/100 Loss: 0.33, Accuracy: 97.52181781901515\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset8/100 Loss: 0.047, Accuracy: 96.92843003582982\n","Validset8/100 Loss: 0.601, Accuracy: 96.31172600392274\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset9/100 Loss: 0.078, Accuracy: 97.413362782955\n","Validset9/100 Loss: 0.382, Accuracy: 96.37473648882133\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset10/100 Loss: 0.086, Accuracy: 96.41409093802952\n","Validset10/100 Loss: 0.42, Accuracy: 95.58826189339338\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset11/100 Loss: 0.043, Accuracy: 97.55705420201569\n","Validset11/100 Loss: 0.323, Accuracy: 97.42300367921523\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset12/100 Loss: 0.031, Accuracy: 98.95480805920492\n","Validset12/100 Loss: 0.34, Accuracy: 97.93943549736368\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset13/100 Loss: 0.066, Accuracy: 98.18095552718901\n","Validset13/100 Loss: 0.509, Accuracy: 96.5422021135266\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset14/100 Loss: 0.036, Accuracy: 98.53639072387075\n","Validset14/100 Loss: 0.462, Accuracy: 97.55359649878476\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset15/100 Loss: 0.05, Accuracy: 98.79574549108268\n","Validset15/100 Loss: 0.81, Accuracy: 95.48560842782518\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset16/100 Loss: 0.11, Accuracy: 96.95674909030454\n","Validset16/100 Loss: 0.531, Accuracy: 94.42751430186763\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset17/100 Loss: 0.125, Accuracy: 95.55822284230538\n","Validset17/100 Loss: 0.501, Accuracy: 94.11807368659133\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset18/100 Loss: 0.075, Accuracy: 96.32852332256424\n","Validset18/100 Loss: 0.39, Accuracy: 95.50093545241654\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset19/100 Loss: 0.065, Accuracy: 98.07958706113727\n","Validset19/100 Loss: 0.449, Accuracy: 96.5442368591507\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset20/100 Loss: 0.046, Accuracy: 98.93080450096488\n","Validset20/100 Loss: 0.505, Accuracy: 96.5267744528235\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset21/100 Loss: 0.107, Accuracy: 97.51647346157382\n","Validset21/100 Loss: 0.632, Accuracy: 94.45350050857604\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset22/100 Loss: 0.077, Accuracy: 98.34347390337298\n","Validset22/100 Loss: 0.848, Accuracy: 95.23686160611983\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset23/100 Loss: 0.079, Accuracy: 96.14286764734048\n","Validset23/100 Loss: 0.522, Accuracy: 95.46976496080532\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset24/100 Loss: 0.105, Accuracy: 96.53943409809482\n","Validset24/100 Loss: 0.365, Accuracy: 95.73479590526327\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset25/100 Loss: 0.043, Accuracy: 99.24271936530302\n","Validset25/100 Loss: 0.419, Accuracy: 97.20816308204994\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset26/100 Loss: 0.07, Accuracy: 97.96248313447501\n","Validset26/100 Loss: 0.354, Accuracy: 96.22895638691134\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset27/100 Loss: 0.069, Accuracy: 98.60379773884546\n","Validset27/100 Loss: 0.266, Accuracy: 97.4109796995822\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset28/100 Loss: 0.053, Accuracy: 98.22118869242286\n","Validset28/100 Loss: 0.411, Accuracy: 97.3457104487007\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset29/100 Loss: 0.089, Accuracy: 98.33106913933076\n","Validset29/100 Loss: 0.536, Accuracy: 95.69468795597152\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset30/100 Loss: 0.052, Accuracy: 98.87951079631166\n","Validset30/100 Loss: 0.447, Accuracy: 96.6336440955522\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset31/100 Loss: 0.068, Accuracy: 97.34826320900878\n","Validset31/100 Loss: 0.372, Accuracy: 96.74296671475442\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset32/100 Loss: 0.08, Accuracy: 97.95307362949549\n","Validset32/100 Loss: 0.38, Accuracy: 95.7630601631088\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset33/100 Loss: 0.061, Accuracy: 97.5624994330434\n","Validset33/100 Loss: 0.323, Accuracy: 97.40517836207539\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset34/100 Loss: 0.063, Accuracy: 98.4051821927221\n","Validset34/100 Loss: 0.295, Accuracy: 97.06924835047401\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset35/100 Loss: 0.036, Accuracy: 99.09768279706752\n","Validset35/100 Loss: 0.331, Accuracy: 98.43102045728625\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset36/100 Loss: 0.072, Accuracy: 98.93864667118778\n","Validset36/100 Loss: 0.451, Accuracy: 97.25300662146269\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset37/100 Loss: 0.094, Accuracy: 97.43481163173892\n","Validset37/100 Loss: 0.546, Accuracy: 96.02964143944205\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset38/100 Loss: 0.092, Accuracy: 95.42932256614603\n","Validset38/100 Loss: 0.776, Accuracy: 95.05255244984819\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset39/100 Loss: 0.085, Accuracy: 97.55699128973394\n","Validset39/100 Loss: 0.492, Accuracy: 95.9627058080702\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset40/100 Loss: 0.075, Accuracy: 96.94857802421744\n","Validset40/100 Loss: 0.522, Accuracy: 96.42759087909023\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset41/100 Loss: 0.062, Accuracy: 98.23512735043779\n","Validset41/100 Loss: 0.522, Accuracy: 96.4477337431891\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset42/100 Loss: 0.076, Accuracy: 96.73392847989003\n","Validset42/100 Loss: 0.482, Accuracy: 96.47753707855959\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset43/100 Loss: 0.075, Accuracy: 98.7948958581709\n","Validset43/100 Loss: 0.652, Accuracy: 95.83576202787387\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset44/100 Loss: 0.045, Accuracy: 99.07127553274522\n","Validset44/100 Loss: 0.637, Accuracy: 96.47774415054542\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset45/100 Loss: 0.055, Accuracy: 98.18371543326664\n","Validset45/100 Loss: 0.718, Accuracy: 95.8597470913536\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset46/100 Loss: 0.052, Accuracy: 97.55832825673983\n","Validset46/100 Loss: 0.535, Accuracy: 96.77243695117038\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset47/100 Loss: 0.065, Accuracy: 98.02661696446819\n","Validset47/100 Loss: 0.39, Accuracy: 96.64862075373645\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset48/100 Loss: 0.06, Accuracy: 98.37683211631021\n","Validset48/100 Loss: 0.467, Accuracy: 96.56159473024323\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset49/100 Loss: 0.061, Accuracy: 98.60535524505829\n","Validset49/100 Loss: 0.704, Accuracy: 96.78393872819801\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset50/100 Loss: 0.057, Accuracy: 98.18275438519974\n","Validset50/100 Loss: 0.52, Accuracy: 97.00827880684984\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset51/100 Loss: 0.044, Accuracy: 98.83392742326595\n","Validset51/100 Loss: 0.557, Accuracy: 97.36415867005348\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset52/100 Loss: 0.108, Accuracy: 96.87363956268456\n","Validset52/100 Loss: 0.737, Accuracy: 94.62775105725471\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset53/100 Loss: 0.1, Accuracy: 96.1421306767706\n","Validset53/100 Loss: 0.581, Accuracy: 95.22007450156056\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset54/100 Loss: 0.075, Accuracy: 98.24405711970546\n","Validset54/100 Loss: 0.748, Accuracy: 95.20542531065723\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset55/100 Loss: 0.06, Accuracy: 98.2623346275235\n","Validset55/100 Loss: 0.601, Accuracy: 96.64988930944976\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset56/100 Loss: 0.049, Accuracy: 97.86908076892088\n","Validset56/100 Loss: 0.558, Accuracy: 96.59809033836336\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset57/100 Loss: 0.041, Accuracy: 98.92089655753533\n","Validset57/100 Loss: 0.524, Accuracy: 97.34534758356845\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset58/100 Loss: 0.05, Accuracy: 98.49935279223982\n","Validset58/100 Loss: 0.567, Accuracy: 96.95513696915135\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset59/100 Loss: 0.06, Accuracy: 98.53565754157088\n","Validset59/100 Loss: 0.93, Accuracy: 95.60174615984249\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset60/100 Loss: 0.073, Accuracy: 97.14181354596819\n","Validset60/100 Loss: 0.628, Accuracy: 95.13109938849954\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset61/100 Loss: 0.039, Accuracy: 98.56652374602321\n","Validset61/100 Loss: 0.459, Accuracy: 96.9453229099536\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset62/100 Loss: 0.029, Accuracy: 99.32824435972715\n","Validset62/100 Loss: 0.417, Accuracy: 98.10787018571182\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset63/100 Loss: 0.045, Accuracy: 99.19340199705162\n","Validset63/100 Loss: 0.574, Accuracy: 96.33383503456275\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset64/100 Loss: 0.042, Accuracy: 98.80447842463606\n","Validset64/100 Loss: 0.587, Accuracy: 97.38493971735443\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset65/100 Loss: 0.051, Accuracy: 98.76404781461451\n","Validset65/100 Loss: 0.553, Accuracy: 97.49569790353718\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset66/100 Loss: 0.039, Accuracy: 98.25165605460484\n","Validset66/100 Loss: 0.407, Accuracy: 97.8071625967367\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset67/100 Loss: 0.047, Accuracy: 99.2315081653237\n","Validset67/100 Loss: 0.543, Accuracy: 96.92710635150338\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset68/100 Loss: 0.052, Accuracy: 97.94826317568425\n","Validset68/100 Loss: 0.693, Accuracy: 96.81966843920051\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset69/100 Loss: 0.098, Accuracy: 97.49213609080326\n","Validset69/100 Loss: 0.435, Accuracy: 96.11576560123024\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset70/100 Loss: 0.107, Accuracy: 96.91239346522897\n","Validset70/100 Loss: 0.595, Accuracy: 95.40175566483204\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset71/100 Loss: 0.033, Accuracy: 99.21875289506285\n","Validset71/100 Loss: 0.441, Accuracy: 97.18127587565657\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset72/100 Loss: 0.045, Accuracy: 97.9809411998084\n","Validset72/100 Loss: 0.49, Accuracy: 96.7352726208024\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset73/100 Loss: 0.039, Accuracy: 99.01621745476368\n","Validset73/100 Loss: 0.358, Accuracy: 97.12888240138453\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset74/100 Loss: 0.037, Accuracy: 98.30500883755602\n","Validset74/100 Loss: 0.585, Accuracy: 95.66559684036774\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset75/100 Loss: 0.066, Accuracy: 97.45541446696323\n","Validset75/100 Loss: 0.512, Accuracy: 95.82041443819298\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset76/100 Loss: 0.062, Accuracy: 98.11702685597452\n","Validset76/100 Loss: 0.62, Accuracy: 95.32225057052928\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset77/100 Loss: 0.056, Accuracy: 98.04996870639658\n","Validset77/100 Loss: 0.588, Accuracy: 95.9384314525683\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset78/100 Loss: 0.065, Accuracy: 96.62018110908363\n","Validset78/100 Loss: 0.553, Accuracy: 95.88497169705958\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset79/100 Loss: 0.07, Accuracy: 98.60421728923977\n","Validset79/100 Loss: 0.523, Accuracy: 96.56766147528968\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset80/100 Loss: 0.047, Accuracy: 97.46267386984502\n","Validset80/100 Loss: 0.509, Accuracy: 96.06815576863116\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset81/100 Loss: 0.085, Accuracy: 97.53831475068488\n","Validset81/100 Loss: 0.551, Accuracy: 95.65815449280393\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset82/100 Loss: 0.051, Accuracy: 99.05517977761662\n","Validset82/100 Loss: 0.451, Accuracy: 96.79679598671729\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset83/100 Loss: 0.081, Accuracy: 98.2756649992345\n","Validset83/100 Loss: 0.579, Accuracy: 95.83957328392559\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset84/100 Loss: 0.052, Accuracy: 98.9695317869295\n","Validset84/100 Loss: 0.644, Accuracy: 96.52358473234963\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset85/100 Loss: 0.063, Accuracy: 98.34589624848623\n","Validset85/100 Loss: 0.51, Accuracy: 96.65910413351253\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset86/100 Loss: 0.038, Accuracy: 98.61837063168097\n","Validset86/100 Loss: 0.572, Accuracy: 97.1819229694237\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset87/100 Loss: 0.08, Accuracy: 98.36967737837546\n","Validset87/100 Loss: 0.753, Accuracy: 95.0675888191828\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset88/100 Loss: 0.057, Accuracy: 98.25915964947652\n","Validset88/100 Loss: 0.637, Accuracy: 96.06206398269921\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset89/100 Loss: 0.052, Accuracy: 98.40579071430513\n","Validset89/100 Loss: 0.474, Accuracy: 97.11062124442996\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset90/100 Loss: 0.028, Accuracy: 99.11489256580964\n","Validset90/100 Loss: 0.513, Accuracy: 97.90877558384699\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset91/100 Loss: 0.04, Accuracy: 98.98785599723118\n","Validset91/100 Loss: 0.503, Accuracy: 97.42544829065291\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset92/100 Loss: 0.025, Accuracy: 99.23831288313839\n","Validset92/100 Loss: 0.377, Accuracy: 98.04733270430573\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset93/100 Loss: 0.039, Accuracy: 98.4502740863425\n","Validset93/100 Loss: 0.453, Accuracy: 97.21569004237315\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset94/100 Loss: 0.069, Accuracy: 98.24931453659262\n","Validset94/100 Loss: 0.728, Accuracy: 95.91311250418767\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset95/100 Loss: 0.057, Accuracy: 98.81708058144982\n","Validset95/100 Loss: 0.682, Accuracy: 96.63820249554405\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset96/100 Loss: 0.055, Accuracy: 97.00343577181164\n","Validset96/100 Loss: 0.946, Accuracy: 95.95405982945087\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset97/100 Loss: 0.062, Accuracy: 97.95098503802241\n","Validset97/100 Loss: 0.964, Accuracy: 95.99708448714773\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset98/100 Loss: 0.099, Accuracy: 95.5689770528226\n","Validset98/100 Loss: 0.619, Accuracy: 94.73767364119934\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","trainset99/100 Loss: 0.05, Accuracy: 98.34746192018191\n","Validset99/100 Loss: 0.576, Accuracy: 96.4024186378164\n","!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"]}]},{"cell_type":"markdown","source":["# 시각화"],"metadata":{"id":"n6zSn57p4hJw"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import matplotlib\n","import seaborn as sns\n","import numpy as np\n","\n","flg, axes = plt.subplots(nrows=1, ncols=3,figsize= (15,15))\n","\n","axes[0].plot(np.arange(len(train_acc_lst), train_acc_lst), label= train_acc)\n","axes[0].plot(np.arange(len(test_acc_lst), test_acc_lst), label= test_acc)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":938},"id":"MvnFBAFw4fJi","executionInfo":{"status":"error","timestamp":1678731171637,"user_tz":-540,"elapsed":579,"user":{"displayName":"김도훈","userId":"04279245867298114899"}},"outputId":"fff5c6b1-da2f-49af-d8e4-9200ed44f825"},"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-da6da25818ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mflg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'list' and 'int'"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x1080 with 3 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":[],"metadata":{"id":"_uaoij6D4jL5"},"execution_count":null,"outputs":[]}]}